{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOfGlTRwvREg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNKk7P6_vUMX"
      },
      "outputs": [],
      "source": [
        "!pip install preprocess\n",
        "!pip install -U mxnet-cu101==1.7.0\n",
        "!pip install d2l==1.0.0-alpha0\n",
        "!pip install h5py pyyaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AllCWdNy45L"
      },
      "source": [
        "Import requiered libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJJquGHave81"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import preprocess as pp\n",
        "import cv2\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Activation\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "\n",
        "import  PIL.Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGNEUx_wzEUN"
      },
      "source": [
        "Using tf.config.list_physical_devices('GPU') to confirm that TensorFlow is using the GPU.\n",
        "\n",
        "Also limiting the GPU consumption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQMidZDZQBWp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "print(\"Num GPUs Available: \", len(physical_devices))\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Se-dESee0l5a"
      },
      "source": [
        "Import the labeled dataset from  NASAâ€™s LAADS DAAC\n",
        "archives. \n",
        "\n",
        "Fast Fourier Transform has been applied to the images in the dataset for denoising."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kC7XqyjBvhhW"
      },
      "outputs": [],
      "source": [
        "data_path = '/path/to.dir/'\n",
        "os.chdir(data_path)\n",
        "print(os.path.abspath(os.getcwd()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A8_7pz61YRq"
      },
      "source": [
        "Loading the pre-trained model Inception V3, which is trained with imagenet dataset.\n",
        "\n",
        "All layers in the Inception V3 has been assigned to not be trainable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBhVkfO1vjwX"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import regularizers\n",
        "from keras.models import Model\n",
        "\n",
        "base_model = InceptionV3(input_shape = (256,256,3), include_top = False, weights = 'imagenet')\n",
        "\n",
        "# Freeze all the layers\n",
        "for layer in base_model.layers:\n",
        "  layer.trainable = False\n",
        "\n",
        "#base_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IRVaga-1xlf"
      },
      "source": [
        "Split the model up to the module mixed7.\n",
        "\n",
        "Assigned 'mixed7' as the last layer from the model, thus the rest of the layers won't be used during training or evaluation.\n",
        "\n",
        "The reason for that is because for the Inception Model V3, the largest feature map is produced by this layer, and further layers have lower dimensions. So for the purpose of feature extraction, we want to use the feature map with the most dimensions possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCQlVKBovmL8"
      },
      "outputs": [],
      "source": [
        "last_layer = base_model.get_layer('mixed7')\n",
        "print('last layer output shape: ', last_layer.output_shape)\n",
        "last_output = last_layer.output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeeujyT22WYx"
      },
      "source": [
        "Applied regularizers to all Conv2D layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x69MD0mWvnK4"
      },
      "outputs": [],
      "source": [
        "for layer in base_model.layers:\n",
        "  if layer.__class__.__name__ == 'Conv2D':\n",
        "    layer.kernel_regularizer = keras.regularizers.l1(0.0001)\n",
        "    layer.kernel_regularizer = keras.regularizers.l2(0.0001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWZVnfwdYAKO"
      },
      "source": [
        "Defining the custom trainable layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-_mwxyFOLs5"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import activations\n",
        "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
        "from tensorflow.keras.layers import Conv2D, Convolution2D, MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "def model_call():\n",
        "\n",
        "  x = layers.Flatten()(last_output) #mixed 7\n",
        "\n",
        "  # Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "\n",
        "  x = layers.Dense(1024, activation='relu')(x)\n",
        "\n",
        "  x = layers.Dropout(0.3)(x)      \n",
        "\n",
        "  # Add a final sigmoid layer for classification\n",
        "  x = layers.Dense(1, activation='sigmoid')(x)         \n",
        "\n",
        "  model = Model(base_model.input, x)\n",
        "\n",
        "  model.compile(optimizer = Adam(lr=1e-04),\n",
        "  #model.compile(optimizer = Adam(lr=1e-06),\n",
        "                loss = 'binary_crossentropy', \n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9tUmSzLqHfA"
      },
      "outputs": [],
      "source": [
        "#model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw4guykK2d02"
      },
      "source": [
        "This callback function will simply prompt, during training, your validation accuracy % \n",
        "\n",
        "if validation accuracy achieves over 90%, 93% it will be notified during training.\n",
        "if validation accuracy is higher than 94%, it will stop training.\n",
        "\n",
        "For training, if you achieve 100% or 95% training accuracy, it will be notified during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoPI007BXBn6"
      },
      "outputs": [],
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "\n",
        "    if(logs.get('accuracy')>0.99):\n",
        "      print(\"\\nReached 99% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = False\n",
        "\n",
        "    elif(logs.get('val_accuracy')>0.9499):\n",
        "      print(\"\\nReached 95% or more validation accuracy, training finished.\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "    elif(logs.get('val_accuracy')>0.93):\n",
        "      print(f\"\\nReached 93% validation accuracy mark!!!\")\n",
        "      \n",
        "    elif(logs.get('val_accuracy')>0.8999):\n",
        "      print(f\"\\nReached 90% validation accuracy mark!\")\n",
        "\n",
        "    elif(logs.get('accuracy')>0.9499):\n",
        "      print(f\"\\nReached 95%  accuracy!\")\n",
        "        \n",
        "accstop = myCallback()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyQUkyDG3CBF"
      },
      "source": [
        "Defined the data generators for the train class, test class and validation class respectively.\n",
        "\n",
        "Also applied Data Augmentation.\n",
        "\n",
        "Class mode is set to binary since we are interested only in two types of output, true or false.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYcH07DPvptN"
      },
      "outputs": [],
      "source": [
        "batch_size = 27 #Defined batch size\n",
        "#batch_size = 140 #Defined batch size\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "        width_shift_range = 0.2,\n",
        "        rescale=1/255,\n",
        "        shear_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        )\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "        rescale=1/255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        './train',\n",
        "        target_size=(256, 256),\n",
        "        #batch_size=80,\n",
        "        batch_size= batch_size,\n",
        "        #shuffle = True,\n",
        "        class_mode='binary')\n",
        "\n",
        "train_list = []\n",
        "batch_index = 0\n",
        "while batch_index <= train_generator.batch_index:\n",
        "  train = train_generator.next()\n",
        "  train_list.append(train[0])\n",
        "  batch_index = batch_index + 1\n",
        "\n",
        "train_array = np.asarray(train_list)\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "        './validation',\n",
        "        target_size=(256, 256),\n",
        "        #batch_size= 24,\n",
        "        batch_size= 20,\n",
        "        #shuffle = True,\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_list = []\n",
        "batch_index = 0\n",
        "while batch_index <= validation_generator.batch_index:\n",
        "  validation = validation_generator.next()\n",
        "  validation_list.append(validation[0])\n",
        "  batch_index = batch_index + 1\n",
        "\n",
        "validation_array = np.asarray(validation_list)\n",
        "\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "        './test',\n",
        "        target_size=(256, 256),\n",
        "        batch_size= batch_size,\n",
        "        class_mode= 'binary')\n",
        "\n",
        "test_list = []\n",
        "batch_index = 0\n",
        "while batch_index <= test_generator.batch_index:\n",
        "  test = test_generator.next()\n",
        "  test_list.append(test[0])\n",
        "  batch_index = batch_index + 1 \n",
        "\n",
        "test_array = np.asarray(test_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63l-3b6fYQhq"
      },
      "source": [
        "This loop will show how the data augmentation generated new images from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4J3-boDNXcCo"
      },
      "outputs": [],
      "source": [
        "for i in range(9):\n",
        "    plt.subplot(330 + 1 + i)\n",
        "    images, labels = train_generator.next()\n",
        "    image = (images[0]*255).astype('uint8')\n",
        "    plt.imshow(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "408qHSwbYaOM"
      },
      "source": [
        "This function allows you to visualize the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hyr5MYInMutR"
      },
      "outputs": [],
      "source": [
        "model1 = model_call()\n",
        "\n",
        "plot_model(model1, to_file='model_plot.png', show_shapes=True, show_layer_names=True) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgtvk3ZeNQ8F"
      },
      "source": [
        "First, we defined a callback for the Learning Rate Scheduler, where it will lower learning rate if there is no improvemnt on validation loss during training.\n",
        "\n",
        "After that, defined empty lists to store the scores from the validation and loss of the test generator during training. \n",
        "\n",
        "Also, the scores from training accuracy, loss and validation accuracy, loss are stored.\n",
        "\n",
        "And Finally, the highest validation scores and lowest loss scores are recorded.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6i2f2FJx7dHp"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import callbacks\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
        "                              patience=20, min_lr= 1e-15)\n",
        "\n",
        "test_acc_per_fold = []; test_loss_per_fold = []\n",
        "\n",
        "model = model_call()\n",
        "epoch = 300\n",
        "\n",
        "kh = model.fit(        \n",
        "        train_generator,\n",
        "        steps_per_epoch= 3,\n",
        "        #steps_per_epoch= 120 // batch_size,\n",
        "        epochs = epoch,\n",
        "        validation_data=validation_generator,\n",
        "        callbacks = [accstop, reduce_lr],\n",
        "        validation_steps= 1\n",
        "        #validation_steps= 32 // batch_size\n",
        "        )\n",
        "   \n",
        "scores = model.evaluate(train_generator, verbose=0)\n",
        "\n",
        "results = model.evaluate(validation_generator, verbose = 0)\n",
        "\n",
        "test_results = model.evaluate(test_generator, verbose = 0)\n",
        "\n",
        "best_acc = max(kh.history['accuracy']); best_loss = min(kh.history['loss']); best_val_acc = max(kh.history['val_accuracy']); best_val_loss = min(kh.history['val_loss'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9490nMl7dNN"
      },
      "outputs": [],
      "source": [
        "print('-----------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
        "\n",
        "print(f'Training Score : {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%;  \\n' )\n",
        "\n",
        "print(f'Validation Score: {model.metrics_names[0]} of {results[0]}; validation {model.metrics_names[1]} of {results[1]*100}%;  \\n' )\n",
        "\n",
        "print(f'Test Score: test {model.metrics_names[0]} of {test_results[0]}; test {model.metrics_names[1]} of {test_results[1]*100}%;  \\n' )\n",
        "\n",
        "print(f' Best Accuracy: {best_acc*100}%;  Best Loss: {best_loss}; Best Val_Accuracy: {best_val_acc*100}%; Best Val_Loss: {best_val_loss} \\n' )\n",
        "\n",
        "print('-----------------------------------------------------------------------------------------------------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6BZIQnN7dRv"
      },
      "outputs": [],
      "source": [
        "plt.plot(kh.history['accuracy'])\n",
        "plt.plot(kh.history['val_accuracy'])\n",
        "plt.title('Training and Validation Accuracies')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc = 'upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G72Rc-pVWU2f"
      },
      "outputs": [],
      "source": [
        "plt.plot(kh.history['loss'])\n",
        "plt.plot(kh.history['val_loss'])\n",
        "plt.title('Training and Validation Losses')\n",
        "plt.ylabel('Losses')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc = 'upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LYQfvxDY-fj"
      },
      "source": [
        "Shows evaluation score of the test generator after training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YETTcYo0WU8e"
      },
      "outputs": [],
      "source": [
        "_, acc = model.evaluate(test_generator)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05ZopiQIZDcs"
      },
      "source": [
        "Shows evaluation score of the validation generator after training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eD4g_Tb6WVBo"
      },
      "outputs": [],
      "source": [
        "_, acc = model.evaluate(validation_generator)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yo032wqgZHOm"
      },
      "source": [
        "This function allows to calculare presicion, recall, accuracy and f1 scores after training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYLi3YLT7dgH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy\n",
        "\n",
        "pre = Precision()\n",
        "re = Recall()\n",
        "acc = BinaryAccuracy()\n",
        "\n",
        "for batch in validation:\n",
        "  X,y = validation\n",
        "  yhat = model.predict(X)\n",
        "  pre.update_state(y, yhat)\n",
        "  re.update_state(y, yhat)\n",
        "  acc.update_state(y, yhat)\n",
        "\n",
        "f1_score = (2 * pre.result().numpy() * re.result())/(pre.result().numpy() + re.result())\n",
        "\n",
        "print('For Validation: ')\n",
        "print(f'Precision: {pre.result().numpy()}, Recall: {re.result().numpy()}, Accuracy: {acc.result().numpy()}, F1 Score: {f1_score}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lzz3mtivWqB2"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy\n",
        "\n",
        "pre = Precision()\n",
        "re = Recall()\n",
        "acc = BinaryAccuracy()\n",
        "\n",
        "for batch in test:\n",
        "  X,y = test\n",
        "  yhat = model.predict(X)\n",
        "  pre.update_state(y, yhat)\n",
        "  re.update_state(y, yhat)\n",
        "  acc.update_state(y, yhat)\n",
        "\n",
        "f1_score = (2 * pre.result().numpy() * re.result())/(pre.result().numpy() + re.result())\n",
        "\n",
        "print('For Test: ')\n",
        "print(f'Precision: {pre.result().numpy()}, Recall: {re.result().numpy()}, Accuracy: {acc.result().numpy()}, F1 Score: {f1_score}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6-7zTrrZRDU"
      },
      "source": [
        "This function allows to calculate prediction valuues for test and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1M9nNTs-WqHf"
      },
      "outputs": [],
      "source": [
        "pred = model.predict(validation_generator)\n",
        "test_pred = model.predict(test_generator)\n",
        "\n",
        "#pred = model.predict( test_generator )\n",
        "\n",
        "print(\"validation predictions:\\n\")\n",
        "\n",
        "def countpredictions(predictions, gw = 0, ngw = 0):\n",
        "  for pred in predictions:\n",
        "    if pred > 0.5:\n",
        "      ngw += 1\n",
        "    else:\n",
        "      gw += 1\n",
        "  return gw, ngw\n",
        "\n",
        "print(countpredictions(pred, 0, 0))\n",
        "\n",
        "print(\"\\ntest predictions:\\n\")\n",
        "\n",
        "def countpredictions(predictions, gw = 0, ngw = 0):\n",
        "  for test_pred in predictions:\n",
        "    if test_pred > 0.5:\n",
        "      ngw += 1\n",
        "    else:\n",
        "      gw += 1\n",
        "  return gw, ngw\n",
        "\n",
        "print(countpredictions(test_pred, 0, 0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3Bhd5_cZigZ"
      },
      "source": [
        "This function allows to calculate Confusion Matrix after training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdEORBtVWqMK"
      },
      "outputs": [],
      "source": [
        "import sklearn.metrics as metrics\n",
        "\n",
        "print(\"Confusion Matrix: \\n\")\n",
        "true_classes = validation_generator.classes\n",
        "class_labels = list(validation_generator.class_indices.keys())\n",
        "pred = np.round(pred)\n",
        "confusion_matrix = metrics.confusion_matrix(y_true=true_classes, y_pred=pred)\n",
        "confusion_matrix\n",
        "\n",
        "#true positives, false negatives\n",
        "#false positives, tru negatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMe_8JWcZnSS"
      },
      "source": [
        "Visualize the learning rate changes that happened during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dju8x86wZcM0"
      },
      "outputs": [],
      "source": [
        "nb_epoch = len(kh.history['loss'])\n",
        "learning_rate=kh.history['lr']\n",
        "xc=range(nb_epoch)\n",
        "plt.figure(3,figsize=(7,5))\n",
        "plt.plot(xc,learning_rate)\n",
        "plt.xlabel('num of Epochs')\n",
        "plt.ylabel('learning rate')\n",
        "plt.title('Learning rate')\n",
        "plt.grid(True)\n",
        "plt.style.use(['seaborn-ticks'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model in h5 format`\n",
        "model.save(\"custom_changing_lr.h5\")"
      ],
      "metadata": {
        "id": "cdnOslglHnPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model.\n",
        "loaded_model = keras.models.load_model(\"custom_changing_lr.h5\")"
      ],
      "metadata": {
        "id": "iJyBzCusGkRo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The loaded model comes compiled and the optimizer\n",
        "# state has been retrained, so training can resume:\n",
        "\n",
        "test_acc_per_fold = []; test_loss_per_fold = []\n",
        "\n",
        "kh = loaded_model.fit(        \n",
        "        train_generator,\n",
        "        steps_per_epoch= 3,\n",
        "        #steps_per_epoch= 120 // batch_size,\n",
        "        epochs = epoch,\n",
        "        validation_data=validation_generator,\n",
        "        callbacks = [accstop, reduce_lr],\n",
        "        validation_steps= 1\n",
        "        #validation_steps= 32 // batch_size\n",
        "        )\n",
        "\n",
        "scores = loaded_model.evaluate(train_generator, verbose=0)\n",
        "\n",
        "results = loaded_model.evaluate(validation_generator, verbose = 0)\n",
        "\n",
        "test_results = loaded_model.evaluate(test_generator, verbose = 0)\n",
        "\n",
        "best_acc = max(kh.history['accuracy']); best_loss = min(kh.history['loss']); best_val_acc = max(kh.history['val_accuracy']); best_val_loss = min(kh.history['val_loss'])"
      ],
      "metadata": {
        "id": "jTHa-06SIQi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('-----------------------------------------------------------------------------------------------------------------------------------------------------------------')\n",
        "\n",
        "print(f'Training Score : {loaded_model.metrics_names[0]} of {scores[0]}; {loaded_model.metrics_names[1]} of {scores[1]*100}%;  \\n' )\n",
        "\n",
        "print(f'Validation Score: {loaded_model.metrics_names[0]} of {results[0]}; validation {loaded_model.metrics_names[1]} of {results[1]*100}%;  \\n' )\n",
        "\n",
        "print(f'Test Score: test {loaded_model.metrics_names[0]} of {test_results[0]}; test {loaded_model.metrics_names[1]} of {test_results[1]*100}%;  \\n' )\n",
        "\n",
        "print(f' Best Accuracy: {best_acc*100}%;  Best Loss: {best_loss}; Best Val_Accuracy: {best_val_acc*100}%; Best Val_Loss: {best_val_loss} \\n' )\n",
        "\n",
        "print('-----------------------------------------------------------------------------------------------------------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "qUTL1f1eIQl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(kh.history['accuracy'])\n",
        "plt.plot(kh.history['val_accuracy'])\n",
        "plt.title('Training and Validation Accuracies')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc = 'upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9LK3Uz3yIQoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(kh.history['loss'])\n",
        "plt.plot(kh.history['val_loss'])\n",
        "plt.title('Training and Validation Losses')\n",
        "plt.ylabel('Losses')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc = 'upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FxRo1NDVJZTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, acc = loaded_model.evaluate(test_generator)\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "sHH--NhWJZWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, acc = loaded_model.evaluate(validation_generator)\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "Gp93AS5aJZaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre = Precision()\n",
        "re = Recall()\n",
        "acc = BinaryAccuracy()\n",
        "\n",
        "for batch in validation:\n",
        "  X,y = validation\n",
        "  yhat = loaded_model.predict(X)\n",
        "  pre.update_state(y, yhat)\n",
        "  re.update_state(y, yhat)\n",
        "  acc.update_state(y, yhat)\n",
        "\n",
        "f1_score = (2 * pre.result().numpy() * re.result())/(pre.result().numpy() + re.result())\n",
        "\n",
        "print('For Validation: ')\n",
        "print(f'Precision: {pre.result().numpy()}, Recall: {re.result().numpy()}, Accuracy: {acc.result().numpy()}, F1 Score: {f1_score}')"
      ],
      "metadata": {
        "id": "NPU8EGdZOGVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre = Precision()\n",
        "re = Recall()\n",
        "acc = BinaryAccuracy()\n",
        "\n",
        "for batch in test:\n",
        "  X,y = test\n",
        "  yhat = loaded_model.predict(X)\n",
        "  pre.update_state(y, yhat)\n",
        "  re.update_state(y, yhat)\n",
        "  acc.update_state(y, yhat)\n",
        "\n",
        "f1_score = (2 * pre.result().numpy() * re.result())/(pre.result().numpy() + re.result())\n",
        "\n",
        "print('For Test: ')\n",
        "print(f'Precision: {pre.result().numpy()}, Recall: {re.result().numpy()}, Accuracy: {acc.result().numpy()}, F1 Score: {f1_score}')"
      ],
      "metadata": {
        "id": "L9qJ2kEyOJb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = loaded_model.predict(validation_generator)\n",
        "test_pred = loaded_model.predict(test_generator)\n",
        "\n",
        "\n",
        "print(\"validation predictions:\\n\")\n",
        "\n",
        "def countpredictions(predictions, gw = 0, ngw = 0):\n",
        "  for pred in predictions:\n",
        "    if pred > 0.5:\n",
        "      ngw += 1\n",
        "    else:\n",
        "      gw += 1\n",
        "  return gw, ngw\n",
        "\n",
        "print(countpredictions(pred, 0, 0))\n",
        "\n",
        "print(\"\\ntest predictions:\\n\")\n",
        "\n",
        "def countpredictions(predictions, gw = 0, ngw = 0):\n",
        "  for test_pred in predictions:\n",
        "    if test_pred > 0.5:\n",
        "      ngw += 1\n",
        "    else:\n",
        "      gw += 1\n",
        "  return gw, ngw\n",
        "\n",
        "print(countpredictions(test_pred, 0, 0))"
      ],
      "metadata": {
        "id": "-tvsJ7CNONSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Confusion Matrix: \\n\")\n",
        "true_classes = validation_generator.classes\n",
        "class_labels = list(validation_generator.class_indices.keys())\n",
        "pred = np.round(pred)\n",
        "confusion_matrix = metrics.confusion_matrix(y_true=true_classes, y_pred=pred)\n",
        "confusion_matrix"
      ],
      "metadata": {
        "id": "5DCcMkohOwOr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Custom Model Changing LR",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}